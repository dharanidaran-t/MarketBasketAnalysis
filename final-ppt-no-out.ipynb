{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Market Basket Analysis Approach to Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**\n",
    "\n",
    "##### *In these recent years, transaction data have been commonly used as research and analysis objects for researchers. In this study, also, transaction data are to be re-processed/re-explored to generate more valuable information. For instance, information of an item whose sales is the highest. Besides, information can be utilized in regard with the stock addition of that item. Moreover, from transaction data there can be utilized as to the relation of each purchased item inside the customer’ basket. By that information, we can make use of it for effective product display/assortment to attract customers’ interest. The commonly-used application to analyze transaction data customers’ shopping basket is market basket analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.1 Market Basket Analysis*\n",
    "\n",
    "##### *Market Basket Analysis is an **Unsupervised machine learning** algorithm performed on customer behavior whilst shopping at a supermarket through the means identifying association and connections among various items placed by the customers in their shopping baskets. In specific, Market Basket Analysis aims at simultaneously identifying the most frequently-purchased items by customers. Here, item is depicted as several kinds of products in supermarket. Using market basket analysis mode, a knowledge of what are the items oftenly purchased by the customers simultaneously and having an opportunity to be promoted can be obtained. With regards to the objective of market basket analysis mode to decide which products that customers purchase at the same time, whereby the name of this mode is taken from the behavior of the customers in placing shopping products into their shopping baskets or shopping list. Over identifying shopping basket pattern of a customer will significantly be able to help a company in using that information in respect of business strategy needs, one of them is placing the most frequently-purchased products simultaneously into one specific area.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.2 Advantages*\n",
    "\n",
    "\n",
    "##### * Market basket analysis is a data mining technique that analyzes patterns of co-occurrence and determines the strength of the link between products purchased together. We also refer to it as frequent itemset mining or association analysis.\n",
    "\n",
    "##### * Market basket analysis is one of the modes from data mining technique prevalently employed to analyze items/goods in one or more shopping baskets that a customer has in one particular moment.\n",
    "\n",
    "##### * Market basket analysis application ought to be designed and implemented at a supermarket not only owing to being able to help the sales promotion design but also able to be made as a reference to re-manage item stock’ incoming and outcoming in the warehouse. \n",
    "\n",
    "##### * In this study, market analysis application will be implemented at Quick-Pick Departmental Store (Near Pondicherry University Kalapet), in regard with its inability to use transaction data. This application is expected to work well and is able to generate the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.3 Customer Behaviour*\n",
    "\n",
    "##### *Definition of customer behavior is a dynamic interaction between cognition, affection, behavior, and its environment whereby someone performs exchange activities in their regular basis [3]. In view of this statement, there are three significant matters to grasp, namely:*\n",
    "1. Customer behavior bears dynamic characteristic, thus, hard to predict.\n",
    "2. Involving interaction, like cognition, affection, behavior, and the occurrences around \n",
    "customers,\n",
    "3. Involving exchange, like the exchange of item and money from merchant to customer.\n",
    "\n",
    "Four factors that could give a rise to customer purchase in shopping, some of them were:\n",
    "1. Cultural Factor\n",
    "2. Social Factor\n",
    "3. Personal Factor\n",
    "4. Psychological Factor\n",
    "\n",
    "*There were three variables that must be regarded in understanding customer behavior, namely \n",
    "stimulus variable, response variable, and intervening variable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Objective**\n",
    "\n",
    "##### The objective of this study is to achieve the following:\n",
    "\n",
    "1. To take the information from the super market and apply machine learning to predict what we need to do in the future.\n",
    "2. To apply data analysis in our thesis. Where we will have our research process of inspecting, cleansing, transforming and modeling data to discover useful information, informing conclusions, and supporting decision-making.\n",
    "3. To apply data mining, We will be applying MBA approach to machine learning. There we will try to understand the customer's purchase behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Pre-Processing**\n",
    "\n",
    "##### Importing the monthwise sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to import and clean the data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Loading April data\n",
    "url1='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/main/quick%20pick%20data%20month%20wise/april_df.csv'\n",
    "df1=pd.read_csv(url1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading May dataset\n",
    "url2='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/data-preprocessing/quick%20pick%20data%20month%20wise/may_df.csv'\n",
    "df2=pd.read_csv(url2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading June data\n",
    "url3='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/data-preprocessing/quick%20pick%20data%20month%20wise/june_df.csv'\n",
    "df3=pd.read_csv(url3)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading July data\n",
    "url4='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/data-preprocessing/quick%20pick%20data%20month%20wise/july_df.csv'\n",
    "df4=pd.read_csv(url4)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading August dataset\n",
    "url5='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/data-preprocessing/quick%20pick%20data%20month%20wise/august_df.csv'\n",
    "df5=pd.read_csv(url5)\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading September dataset\n",
    "url6='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/data-preprocessing/quick%20pick%20data%20month%20wise/september_df.csv'\n",
    "df6=pd.read_csv(url6)\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading October dataset\n",
    "url7='https://raw.githubusercontent.com/dharanidaran-t/MarketBasketAnalysis/data-preprocessing/quick%20pick%20data%20month%20wise/october_df.csv'\n",
    "df7=pd.read_csv(url7)\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating dataframes row-wise (stack)\n",
    "\n",
    "combined_df = pd.concat([df1, df2, df3, df4, df5, df6, df7], ignore_index=True)\n",
    "# Setting ignore_index=True ensures that the resulting dataframe has a continuous index.\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type formatting\n",
    "\n",
    "combined_df['Date']=pd.to_datetime(combined_df['Date'])\n",
    "combined_df['Item Name']=combined_df['Item Name'].str.strip().astype('str')\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating date,month,day columns from the data using pandas datetime format\n",
    "\n",
    "combined_df['Year'] = combined_df['Date'].dt.year\n",
    "combined_df['Month'] = combined_df['Date'].dt.month_name()\n",
    "combined_df['Day'] = combined_df['Date'].dt.day\n",
    "combined_df['Day of week'] = combined_df['Date'].dt.day_name()\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the additional details\n",
    "\n",
    "combined_df.drop(columns=['Barcode','Net Amount'],axis=1)\n",
    "\n",
    "# Exporting the csv file\n",
    "combined_df.to_csv(r'C:/Users/Win 10/Desktop/Market-Basket-Analysis/groceries_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combined sales df is now saved in a  new csv file (groceries_df)\n",
    "* Now, let us create a new df to analyzes patterns of co-occurrence. In this dataframe, we will display the items purchased together by a Bill No in a Single row (i.e., Displaying the items purchased by a person in a single purchase column-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Bill Number' and create new columns for each item\n",
    "df_grouped = combined_df.groupby('Bill No')['Item Name'].apply(lambda x: pd.Series(x.values)).unstack().reset_index()\n",
    "# Rename columns\n",
    "df_grouped.columns = ['Bill Number'] + [f'{i+1}' for i in range(df_grouped.shape[1]-1)]\n",
    "df_grouped=df_grouped.drop(columns=['Bill Number'])\n",
    "df_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows which has NaN value at the 2nd column (column index 1)\n",
    "df_grouped = df_grouped.dropna(subset=[df_grouped.columns[1]], axis=0)\n",
    "# Drop columns from 11 to 77\n",
    "df_grouped.drop(columns=df_grouped.columns[10:77], inplace=True)\n",
    "df_grouped.head(20)\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to new csv file\n",
    "df_grouped.to_csv(r'C:/Users/Win 10/Desktop/Market-Basket-Analysis/basket_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Exploratory data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "# For importing, cleaning and analysing data\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df=pd.read_csv(r'C:/Users/Win 10/Desktop/Market-Basket-Analysis/groceries_df.csv')\n",
    "groceries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip leading characters- Batch names O,Q and Y and empty spaces\n",
    "\n",
    "import re\n",
    "\n",
    "def rename_10(item_name):\n",
    "    return re.sub(r'^[RS10]\\s*', 'PEN', item_name)\n",
    "\n",
    "groceries_df['Item Name'] = groceries_df['Item Name'].apply(rename_10)\n",
    "\n",
    "def clean_item_name(item_name):\n",
    "    return re.sub(r'^[OQXY]\\s*', '', item_name)\n",
    "\n",
    "# Apply function to 'Item Name' column\n",
    "groceries_df['Item Name'] = groceries_df['Item Name'].apply(clean_item_name)\n",
    "\n",
    "def removenum(item_name):\n",
    "    return re.sub(r'^[\\d.-]+\\s*', '', item_name)\n",
    "\n",
    "# Apply function to 'Item Name' column\n",
    "groceries_df['Item Name'] = groceries_df['Item Name'].apply(removenum)\n",
    "\n",
    "groceries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NaN values\n",
    "\n",
    "groceries_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is no missing or NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping data with negative or zero quantity\n",
    "\n",
    "negativeorNull = groceries_df.loc[groceries_df['Qty']<=0]\n",
    "print('Count of Null or negative quantity: ', len(negativeorNull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping data with zero or negative price\n",
    "\n",
    "negativeorNull = groceries_df.loc[groceries_df['Rate']<=0]\n",
    "print('Count of Null or negative Price: ', len(negativeorNull))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no entry with zero Quantities purchased, data is ready to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'Year' column for unique values\n",
    "\n",
    "groceries_df['Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'Month' column for unique values\n",
    "\n",
    "groceries_df['Month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'Day' column for unusual values\n",
    "\n",
    "groceries_df['Day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'Day of week' column for unusual values\n",
    "\n",
    "groceries_df['Day of week'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find out the time period of the dataset\n",
    "\n",
    "print(f\"The dataset is from dates {groceries_df['Date'].min()} to {groceries_df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries=groceries_df.drop(columns=['Year','Day'],axis=1)\n",
    "groceries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df.groupby(['Year', 'Month']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find how many unique purchases have been done\n",
    "groceries_df['Bill No'].unique()\n",
    "purchases=groceries_df['Bill No'].nunique()\n",
    "print(f\"There are {purchases} unique Purchases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items=groceries_df['Item Name'].unique()\n",
    "Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find how many unique items are there in the dataset\n",
    "\n",
    "n_items=groceries_df['Item Name'].nunique()\n",
    "print(f\"There are {n_items} unique Items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountOfItem = groceries_df['Item Name'].value_counts()\n",
    "\n",
    "sortedItems = CountOfItem.sort_values(ascending=False)\n",
    "\n",
    "df=pd.DataFrame(list(sortedItems.items()),columns=['Item name','Counts'])\n",
    "df\n",
    "print('Top 50 items:\\n',df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find the least 15 purchased item type\n",
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df.groupby(['Month'])['Amount'].sum()\n",
    "print(groceries_df['Month'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df.groupby(['Day of week'])['Amount'].sum()\n",
    "print(groceries_df['Day of week'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find out the top 20 Bill's where most number of items purchased\n",
    "\n",
    "groceries_df['Bill No'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate is the total price column\n",
    "groceries_df.groupby(['Year', 'Month'])['Rate'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "\n",
    "# For visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Importing Dataset\n",
    "groceries_df\n",
    "\n",
    "# RENAME COLUMN\n",
    "groceries_df = groceries_df.rename(columns={'Item Name': 'title'})\n",
    "#Checking the Data\n",
    "\n",
    "groceries_df.head()\n",
    "\n",
    "#Creating the text variable\n",
    "\n",
    "text2 = \" \".join(title for title in groceries_df.title)\n",
    "\n",
    "# Creating word_cloud with text as argument in .generate() method\n",
    "\n",
    "word_cloud2 = WordCloud(collocations = False, background_color = 'white').generate(text2)\n",
    "\n",
    "# Display the generated Word Cloud\n",
    "\n",
    "plt.imshow(word_cloud2, interpolation='bilinear')\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME COLUMN\n",
    "groceries_df = groceries_df.rename(columns={'title': 'Item Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_freq = groceries_df['Item Name'].value_counts().sort_values(ascending = False).head(25)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "item_freq.plot(kind='bar', color='wheat')\n",
    "plt.xlabel('Item Name')\n",
    "plt.ylabel('Frequency (absolute)')\n",
    "plt.title('Top-25 Absolute Item Frequency Plot')\n",
    "plt.show()\n",
    "\n",
    "# top 10 item frequecies\n",
    "item_freq = groceries_df['Item Name'].value_counts().sort_values(ascending = False).head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "item_freq.plot(kind='bar', color='blue')\n",
    "plt.xlabel('Item Name')\n",
    "plt.ylabel('Frequency (absolute)')\n",
    "plt.title('Top-10 Absolute Item Frequency Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find the least 15 purchased item type\n",
    "item_freq = groceries_df['Item Name'].value_counts().sort_values(ascending = False).tail(10).plot(kind='bar', color='seagreen', edgecolor='black')\n",
    "\n",
    "plt.xlabel('Item types')\n",
    "plt.ylabel('Purchase count')\n",
    "plt.title('Least 15 purchased item types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if there is any different in the number of transactions month-wise\n",
    "\n",
    "groceries_df['Month'].value_counts()\n",
    "groceries_df['Month'].value_counts().sort_values(ascending = False).plot(kind='bar',edgecolor='black',color='green')\n",
    "plt.title('Number of transactions month-wise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximum Sale is recorded in August\n",
    "* Least sale is recorded in  July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction per weekday\n",
    "plt.figure(figsize = (8,6))\n",
    "groceries_df.groupby(groceries_df['Day of week'])['Bill No'].nunique().sort_values(ascending = False).plot(kind='bar',color='lightblue')\n",
    "plt.xlabel('Week day')\n",
    "plt.ylabel('Transactions')\n",
    "plt.title('Transactions by Weekday')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Approximately every day records approximately equal number of transactions except on Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest sales amount items (Top 50)\n",
    "\n",
    "import jinja2\n",
    "\n",
    "cm = sns.light_palette('pink', as_cmap = True)\n",
    "item_sales = groceries_df.groupby('Item Name')['Rate'].sum().sort_values(ascending= False)\n",
    "item_sales.to_csv('ItemSales.csv')\n",
    "item_sales = pd.read_csv('ItemSales.csv')\n",
    "item_sales.head(50).style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least sales amount items (Least 20)\n",
    "import jinja2\n",
    "\n",
    "cm = sns.light_palette('pink', as_cmap = True)\n",
    "item_sales = groceries_df.groupby('Item Name')['Rate'].sum().sort_values(ascending= False)\n",
    "item_sales.to_csv('ItemSales.csv')\n",
    "item_sales = pd.read_csv('ItemSales.csv')\n",
    "item_sales.tail(20).style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Research Methodology** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Associate Rule Mining with Apriori Algorithm**\n",
    "\n",
    "##### **What is Apriori?**\n",
    "\n",
    "#### Apriori algorithm uses frequent itemsets to get association rules, but on the assumptions that:\n",
    "\n",
    "*1. All subsets of frequent itemsets must be frequent*\n",
    "\n",
    "*2. Similarly incase of infrequent subset their parent set is infrequent too The algorithm works in such a way that a minimum support value is set and iterations happen with frequent itemsets. Itemsets and subsets are ignored if their support is below the threshold till there can’t be any removal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Association rule is related to the statement of “what’ with what”. This matter can be in a form of statement on transaction activity carried out by the customers at a supermarket. From that statement, there has a strong relation to the study of customer transaction data database to determine the habit of a purchased product with what product, thus, association rule is frequently referred as market basket analysis. The significance of an associative rule can be figured in the presence of two parameters, namely **support** and **confidence**. Support (supporting value) is the percentage of combinations of product items in the database. While confidence (certainty value) is a value to determine the strength of inter-item relationships in association rules.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 We can utilize three core measures that are used in Association Rule Learning, which are: Support, Confidence, and Lift.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **i. Support:**\n",
    "\n",
    " *It signifies the popularity of the item, if an item is less frequently bought then it will be ignored in the association.*\n",
    "\n",
    "\n",
    "### **ii. Confidence:**\n",
    "\n",
    " It tells the likelihood of purchasing Y when X is bought.Sounds more like a conditional probability. Infact it is ! But it fails to check the popularity(frequency) of Y to overcome that we got lift.\n",
    "\n",
    "### **iii. Lift:**\n",
    "\n",
    " It combines both confidence and support.A lift greater than 1 suggests that the presence of the antecedent increases the chances that the consequent will occur in a given transaction. Lift below 1 indicates that purchasing the antecedent reduces the chances of purchasing the consequent in the same transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For importing, cleaning and transforming data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# For data analysis\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket=(groceries_df.groupby(['Bill No','Item Name'])['Qty']\n",
    "        .sum().unstack().reset_index().fillna(0)\n",
    "        .set_index('Bill No'))\n",
    "\n",
    "def encode_unit(x):\n",
    "    if x<= 0:\n",
    "        return 0\n",
    "    if x>= 0:\n",
    "        return 1\n",
    "basket_sets=basket1.map(encode_unit)\n",
    "\n",
    "freq_itemsets=apriori(basket_sets,min_support=0.01,use_colnames=True)\n",
    "print(freq_itemsets)\n",
    "\n",
    "rules=association_rules(freq_itemsets, metric=\"lift\", min_threshold=0.05)\n",
    "\n",
    "# confidence tells us the how likely the consequent will be bought when the antecedents is bought\n",
    "\n",
    "# lift tells us the strength of the rule\n",
    "print(rules.sort_values(by='lift', ascending=False))\n",
    "\n",
    "##################################################################\n",
    "# This code will rise Memory Error - It requires too much memory #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since it takes too much memory to process the whole basket, Lets analyse the baskets month-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket1=(groceries_df[groceries_df['Month']==\"April\"]\n",
    "        .groupby(['Bill No','Item Name'])['Qty']\n",
    "        .sum().unstack().reset_index().fillna(0)\n",
    "        .set_index('Bill No'))\n",
    "\n",
    "def encode_unit(x):\n",
    "    if x<= 0:\n",
    "        return 0\n",
    "    if x>0:\n",
    "        return 1\n",
    "basket1_sets=basket1.map(encode_unit)\n",
    "\n",
    "freq_itemsets_apr=apriori(basket1_sets,min_support=0.01,use_colnames=True)\n",
    "\n",
    "rules_apr=association_rules(freq_itemsets_apr, metric=\"lift\", min_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freq Items\n",
    "freq_itemsets_apr.sort_values(by='support',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence tells us the how likely the consequent will be bought when the antecedents is bought\n",
    "# lift tells us the strength of the rule\n",
    "rule1=rules_apr.sort_values(by='lift', ascending=False).drop(columns=['antecedent support','consequent support','leverage','conviction','zhangs_metric'])\n",
    "rule1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *  *From the April month basket, we infer that Tomato and Onion has been frequently bought together.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets analyse the July month basket - which has registered the lowest sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket3=(groceries_df[groceries_df['Month']==\"July\"]\n",
    "        .groupby(['Bill No','Item Name'])['Qty']\n",
    "        .sum().unstack().reset_index().fillna(0)\n",
    "        .set_index('Bill No'))\n",
    "\n",
    "def encode_unit(x):\n",
    "    if x<= 0:\n",
    "        return 0\n",
    "    if x>0:\n",
    "        return 1\n",
    "basket3_sets=basket3.map(encode_unit)\n",
    "\n",
    "freq_itemsets_july=apriori(basket3_sets,min_support=0.01,use_colnames=True)\n",
    "\n",
    "rules_july=association_rules(freq_itemsets_july, metric=\"lift\", min_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_itemsets_july.sort_values(by='support',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence tells us the how likely the consequent will be bought when the antecedents is bought\n",
    "# lift tells us the strength of the rule\n",
    "rules_july.sort_values(by='lift', ascending=False).drop(columns=['antecedent support','consequent support','leverage','conviction','zhangs_metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *  *From the July month basket, we infer that Tomato-Onion, Green chilli-Tomato has been frequently bought together.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets analyse the August month basket - which has registered the highest sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket4=(groceries_df[groceries_df['Month']==\"August\"]\n",
    "        .groupby(['Bill No','Item Name'])['Qty']\n",
    "        .sum().unstack().reset_index().fillna(0)\n",
    "        .set_index('Bill No'))\n",
    "\n",
    "def encode_unit(x):\n",
    "    if x<= 0:\n",
    "        return 0\n",
    "    if x>0:\n",
    "        return 1\n",
    "basket4_sets=basket4.map(encode_unit)\n",
    "\n",
    "freq_itemsets_aug=apriori(basket4_sets,min_support=0.01,use_colnames=True)\n",
    "freq_itemsets_aug.sort_values(by='support',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_aug.sort_values(by='lift', ascending=False).drop(columns=['antecedent support','consequent support','leverage','conviction','zhangs_metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *   *From the August month basket, we can see that those who bought Pillow have bought the Bed and vice-versa. In this month alone we can see a non grocery item mostly bought in the basket, it maybe the admission time in the university so that newly admitted studednts who are opting for hostel's bought the furniture items.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **To create a basket for the overall sales data, we have cleaned the groceries dataset based on some criteria(Removed the single items & limited the columns to 10 in a single purchase) and reduced the size of the basket so that we can run the overall analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For importing, cleaning and transforming data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data analysis\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Visualise the results\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# importing the basket dataframe\n",
    "basket_df=pd.read_csv(r'C:/Users/Win 10/Desktop/Market-Basket-Analysis/basket_df.csv')\n",
    "\n",
    "# Filling the NaN values with the word 'NA'\n",
    "basket_df.fillna('NA', inplace=True)\n",
    "\n",
    "# Formatting dataframe into list of lists\n",
    "basket_df_list = basket_df.values.tolist()\n",
    "\n",
    "# Removing 'NA' from each list\n",
    "for i in range(len(basket_df_list)):\n",
    "    basket_df_list[i] = [x for x in basket_df_list[i] if not x=='NA']\n",
    "\n",
    "# Transactional encoding\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(basket_df_list).transform(basket_df_list)\n",
    "\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apriori Algorithm\n",
    "\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.003, use_colnames=True)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05)\n",
    "\n",
    "# confidence tells us the how likely the consequent will be bought when the antecedents is bought\n",
    "rules.sort_values(by='confidence', ascending=False).head()\n",
    "\n",
    "# lift tells us the strength of the rule\n",
    "rules.sort_values(by='lift', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.sort_values(by='support',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules.sort_values(by='lift', ascending=False).drop(columns=['antecedent support','consequent support','leverage','conviction','zhangs_metric']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules.sort_values(by='lift', ascending=False).drop(columns=['antecedent support','consequent support','leverage','conviction','zhangs_metric']).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5.3 Visualisation of the Association rules**\n",
    "\n",
    "## 5.3.1Creating a Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(rules, x=rules.index, y='support', text='support', labels={'index': 'Association Rule'})\n",
    "fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')\n",
    "fig.update_layout(title='Association Rules by Support', xaxis_title='Association Rule', yaxis_title='Support')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "\n",
    "\n",
    "x = rules['support']\n",
    "y = rules['confidence']\n",
    "z = rules['lift']\n",
    "\n",
    "ax.set_xlabel(\"Support\")\n",
    "ax.set_ylabel(\"Confidence\")\n",
    "ax.set_zlabel(\"Lift\")\n",
    "\n",
    "ax.scatter(x, y, z)\n",
    "ax.set_title(\"3D Distribution of Association Rules\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Creating Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the style\n",
    "plt.figure(figsize = (15, 15))\n",
    "sns.set_style('darkgrid')\n",
    "#Plotting the relationship between the metrics\n",
    "plt.subplot(2,2,1)\n",
    "sns.scatterplot(x=\"support\", y=\"confidence\",data=rules)\n",
    "plt.subplot(2,2,2)\n",
    "sns.scatterplot(x=\"support\", y=\"lift\",data=rules)\n",
    "plt.subplot(2,2,3)\n",
    "sns.scatterplot(x=\"confidence\", y=\"lift\",data=rules)\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot(x=\"antecedent support\", y=\"consequent support\",data=rules)\n",
    "plt.title('Scatter-Plots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(rules, x='confidence', y='lift', title='Confidence vs Lift')\n",
    "fig.update_traces(marker=dict(size=12, color='skyblue', line=dict(width=2, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
    "fig.update_layout(xaxis_title='Confidence', yaxis_title='Lift', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(rules, x='confidence', y='support', title='Confidence vs Support')\n",
    "fig.update_traces(marker=dict(size=12, color='skyblue', line=dict(width=2, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
    "fig.update_layout(xaxis_title='Confidence', yaxis_title='Support', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_network(rules, rules_to_show):\n",
    "  # Directional Graph from NetworkX\n",
    "  network = nx.DiGraph()\n",
    "  \n",
    "  # Loop through number of rules to show\n",
    "  for i in range(rules_to_show):\n",
    "    \n",
    "    # Add a Rule Node\n",
    "    network.add_nodes_from([\"R\"+str(i)])\n",
    "    for antecedents in rules.iloc[i]['antecedents']: \n",
    "        # Add antecedent node and link to rule\n",
    "        network.add_nodes_from([antecedents])\n",
    "        network.add_edge(antecedents, \"R\"+str(i),  weight = 2)\n",
    "      \n",
    "    for consequents in rules.iloc[i]['consequents']:\n",
    "        # Add consequent node and link to rule\n",
    "        network.add_nodes_from([consequents])\n",
    "        network.add_edge(\"R\"+str(i), consequents,  weight = 2)\n",
    "\n",
    "  color_map=[]  \n",
    "  \n",
    "  # For every node, if it's a rule, colour as Black, otherwise Orange\n",
    "  for node in network:\n",
    "       if re.compile(\"^[R]\\d+$\").fullmatch(node) != None:\n",
    "            color_map.append('black')\n",
    "       else:\n",
    "            color_map.append('orange')\n",
    "  \n",
    "  # Position nodes using spring layout\n",
    "  pos = nx.spring_layout(network, k=16, scale=1)\n",
    "  # Draw the network graph\n",
    "  nx.draw(network, pos, node_color = color_map, font_size=8)            \n",
    "  \n",
    "  # Shift the text position upwards\n",
    "  for p in pos:  \n",
    "      pos[p][1] += 0.12\n",
    "\n",
    "  nx.draw_networkx_labels(network, pos)\n",
    "  plt.title(\"Network Graph for Association Rules\")\n",
    "  plt.show()\n",
    "\n",
    "draw_network(rules, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5.4 Business Application**\n",
    "\n",
    "##### *Let’s say the grocery has bought up too much Egg and is now worrying that the stocks will expire if they cannot be sold out in time. To make matters worse, the profit margin of Whole Milk is so low that they cannot afford to have a promotional discount without killing too much of their profits. One approach that can be proposed is to find out which products drive the sales of Whole Milk and offer discounts on those products instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_rules = rules[rules['consequents'].astype(str).str.contains('EGG')]\n",
    "egg_rules = egg_rules.sort_values(by=['lift'],ascending = [False]).reset_index(drop = True)\n",
    "\n",
    "display(egg_rules.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *For instance, we can apply a promotional discount on Vegetables and Bread. Some of the associations may seem counter-intuitive, but the rules state that these products do drive the sales of Egg.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Inferences & Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA)\n",
    "\n",
    "- **Duration of the Dataset:** The dataset covers a 6-month period.\n",
    "- **Number of Unique Itemsets:** The dataset contains a variety of unique itemsets, including several frequently purchased combinations.\n",
    "- **Number of Purchases:** Over the 6-month period, the dataset recorded a significant number of purchases, showcasing the buying patterns of the customers.\n",
    "- **Weekly Sales Analysis:** Weekly sales data indicates a consistent pattern of purchasing, with certain peaks during weekends and special sale events.\n",
    "- **Monthly Sales Analysis:** Monthly sales data reveals trends such as increased purchases at the beginning and end of each month, possibly due to salary cycles and end-of-month sales.\n",
    "\n",
    "#### Market Basket Analysis using Apriori Algorithm\n",
    "\n",
    "The Apriori algorithm was applied to the dataset to identify frequent itemsets and association rules. The key findings include:\n",
    "\n",
    "- **Onion-Tomato:** This combination was found to be one of the most frequently purchased together, indicating that customers commonly buy these two vegetables simultaneously.\n",
    "- **Green Chilli-Tomato:** Similar to the Onion-Tomato combination, Green Chilli-Tomato pairs are also frequently bought together, suggesting a pattern in the customers' cooking habits.\n",
    "- **Bed-Pillow:** This combination indicates a strong association between these bedding items, reflecting a common shopping behavior for home essentials.\n",
    "\n",
    "The Apriori classification method efficiently identified these frequent itemsets by analyzing the transactions and determining the support and confidence levels of the item pairs.\n",
    "\n",
    "#### Suggestions for the Departmental Store\n",
    "\n",
    "1. **Promotional Bundling:** Create bundled promotions for frequently bought together items like Onion-Tomato and Green Chilli-Tomato. This can encourage customers to buy more and increase sales volume.\n",
    "  \n",
    "2. **Cross-Merchandising:** Place related items like bed and pillows together in the store to make it easier for customers to find and purchase these items together, enhancing their shopping experience.\n",
    "\n",
    "3. **Inventory Management:** Ensure that frequently paired items are always in stock to avoid missed sales opportunities. Regularly monitor inventory levels for these items and adjust stock accordingly.\n",
    "\n",
    "4. **Targeted Marketing:** Utilize the insights from the market basket analysis to design targeted marketing campaigns. For example, send personalized offers to customers who frequently buy onions and tomatoes together.\n",
    "\n",
    "5. **Layout Optimization:** Organize the store layout to reflect the common buying patterns identified. For instance, placing vegetables that are often bought together in close proximity can streamline the shopping process for customers.\n",
    "\n",
    "By implementing these strategies, the departmental store can enhance customer satisfaction, boost sales, and improve overall operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References**\n",
    "\n",
    "1. Raich, B. Ganguly, and M. Tota, \"Machine Learning for Market Basket Analysis through,\" IOSR Journal of Engineering (IOSRJEN), pp. 22-23, 2019. \n",
    "2. S. Mainali, \"MARKET BASKET ANALYSIS,\" GitHub, Kirtipur, 2016.\n",
    "3. https://www.researchgate.net/publication/355894565_Market_Basket_Analysis_Approach_to_Machine_Learning\n",
    "4. https://www.researchgate.net/publication/365489098_MARKET_BASKET_ANALYSIS_FOR_A_SUPERMARKET\n",
    "5. https://www.kaggle.com/code/mukandkrishna/mba-apriori"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
